{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNFDiEkC6bkdpWe1qzgczFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HakureiPOI/Comp_Arch/blob/lab4-5/Arch_Lab_4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***HakureiPOI*** *Arch_Lab_4-5*"
      ],
      "metadata": {
        "id": "PfEXjNjQNV9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## *环境准备*"
      ],
      "metadata": {
        "id": "X6szOjUDNgqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opQ3k9Irk3pU",
        "outputId": "bdb63439-fe49-4455-ccb4-3bbb1b1e2758"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKo3pTgOID9D",
        "outputId": "31f1fe3b-b475-4b76-c1d7-ce5018aec09c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 26 05:55:33 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di2mebnikhbw",
        "outputId": "aafa6711-afa4-4912-ddea-ec047721d03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Comp_Arch'...\n",
            "remote: Enumerating objects: 987, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 987 (delta 35), reused 44 (delta 7), pack-reused 900 (from 2)\u001b[K\n",
            "Receiving objects: 100% (987/987), 62.12 MiB | 17.01 MiB/s, done.\n",
            "Resolving deltas: 100% (604/604), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HakureiPOI/Comp_Arch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Comp_Arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTo4gTFQnGsL",
        "outputId": "5f13eb67-3655-4afb-b390-5ec5d31fa83d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Comp_Arch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp2kKX_YnRqC",
        "outputId": "62cf5acb-8dc6-43e4-a02d-357a1e7da7ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mmain\u001b[m\n",
            "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
            "  \u001b[31mremotes/origin/lab1\u001b[m\n",
            "  \u001b[31mremotes/origin/lab2\u001b[m\n",
            "  \u001b[31mremotes/origin/lab3\u001b[m\n",
            "  \u001b[31mremotes/origin/lab4-5\u001b[m\n",
            "  \u001b[31mremotes/origin/lab6\u001b[m\n",
            "  \u001b[31mremotes/origin/main\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git switch lab4-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgrwmtxSnUAC",
        "outputId": "87afcb52-4b86-4292-c4e9-65e279f4a5fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'lab4-5' set up to track remote branch 'lab4-5' from 'origin'.\n",
            "Switched to a new branch 'lab4-5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lab4-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-98u2u4knbtn",
        "outputId": "b7b9295e-e05c-4f31-d5b3-3755af6b591a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Comp_Arch/lab4-5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## *Lab4*"
      ],
      "metadata": {
        "id": "czChKfb8ROdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### *实现 CUDA 矩阵乘法核函数*\n",
        "\n",
        "* Step 1: 计算行和列索引\n",
        "在核函数中，首先需要计算每个线程负责计算的输出矩阵 `P` 的元素位置。这通过 `blockIdx` 和 `threadIdx` 来实现，它们分别表示线程块和线程在网格中的位置。\n",
        "\n",
        "```cuda\n",
        "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "```\n",
        "\n",
        "这里，`blockDim` 定义了每个线程块的尺寸，`blockIdx` 和 `threadIdx` 用于计算每个线程的全局位置。\n",
        "\n",
        "* Step 2: 确保线程在矩阵边界内\n",
        "在进行计算之前，需要确保线程计算的位置在矩阵的边界内，以避免越界访问。\n",
        "\n",
        "```cuda\n",
        "if (row < width && col < width) {\n",
        "    float pValue = 0.0;\n",
        "```\n",
        "\n",
        "* Step 3: 计算矩阵乘法的单个元素\n",
        "每个线程计算矩阵 `P` 的一个元素，这是通过遍历矩阵 `M` 的列（或矩阵 `N` 的行）来实现的。\n",
        "\n",
        "```cuda\n",
        "for (int k = 0; k < width; ++k) {\n",
        "    pValue += d_M[row * width + k] * d_N[k * width + col];\n",
        "}\n",
        "```\n",
        "\n",
        "这里，`d_M` 和 `d_N` 分别是输入矩阵 `M` 和 `N` 在设备上的存储，`width` 是矩阵的宽度（对于方阵来说，宽度等于高度）。\n",
        "\n",
        "* Step 4: 存储计算结果\n",
        "计算完成后，将结果存储到输出矩阵 `P` 的相应位置。\n",
        "\n",
        "```cuda\n",
        "d_P[row * width + col] = pValue;\n",
        "```\n"
      ],
      "metadata": {
        "id": "iZucsMHplTQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***具体的代码实现如下：***\n",
        "\n",
        "\n",
        "```cuda\n",
        "__global__ void MatrixMulKernel(float *d_M, float *d_N, float *d_P, int width) {\n",
        "  // Calculate the row index of the P element and M\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "  // Calculate the column index of the P element and N\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  // Ensure the thread is within bounds\n",
        "  if (row < width && col < width) {\n",
        "    float pValue = 0.0;\n",
        "\n",
        "    // Each thread computes one element of the matrix\n",
        "    for (int k = 0; k < width; ++k) {\n",
        "      pValue += d_M[row * width + k] * d_N[k * width + col];\n",
        "    }\n",
        "\n",
        "    // Store the computed value into the output matrix\n",
        "    d_P[row * width + col] = pValue;\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "KKQGfo6al_j6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *尝试编译*"
      ],
      "metadata": {
        "id": "HtMKatquRStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=compute_75 -L/usr/local/cuda/lib64 -lcublas ./matrix_mul_lab4.cu"
      ],
      "metadata": {
        "id": "4EppDy2Ynzlh",
        "outputId": "70108fe3-412b-4652-e7ee-a55a2780f631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(198)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"alpha\"\u001b[0m was declared but never referenced\n",
            "    const float alpha = 1.0f;\n",
            "                ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(199)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"beta\"\u001b[0m was declared but never referenced\n",
            "    const float beta = 0.0f;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(18)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"BLOCK_SIZE\"\u001b[0m was declared but never referenced\n",
            "  const int BLOCK_SIZE = TILE_WIDTH;\n",
            "            ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *脚本测试*"
      ],
      "metadata": {
        "id": "oyRzPxhpnRjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash testbench_lab4.sh"
      ],
      "metadata": {
        "id": "W4aPosTqqV8O",
        "outputId": "db38a70e-53bb-43bf-d72a-a5d62bd740c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling with TILE_SIZE=16\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=16\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.061 ms\n",
            "Performance= 69.28 GFlop/s, Time= 0.061 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.176 ms\n",
            "Performance= 190.14 GFlop/s, Time= 0.176 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.149 ms\n",
            "Performance= 233.61 GFlop/s, Time= 1.149 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.124 ms\n",
            "Performance= 235.37 GFlop/s, Time= 9.124 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 58.307 ms\n",
            "Performance= 294.64 GFlop/s, Time= 58.307 msec, Size= 17179869184 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Compiling with TILE_SIZE=32\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=32\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.050 ms\n",
            "Performance= 83.16 GFlop/s, Time= 0.050 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.183 ms\n",
            "Performance= 183.22 GFlop/s, Time= 0.183 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.149 ms\n",
            "Performance= 233.57 GFlop/s, Time= 1.149 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.137 ms\n",
            "Performance= 235.04 GFlop/s, Time= 9.137 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 55.992 ms\n",
            "Performance= 306.83 GFlop/s, Time= 55.992 msec, Size= 17179869184 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Compiling with TILE_SIZE=64\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=64\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.051 ms\n",
            "Performance= 82.45 GFlop/s, Time= 0.051 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.179 ms\n",
            "Performance= 187.20 GFlop/s, Time= 0.179 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.151 ms\n",
            "Performance= 233.18 GFlop/s, Time= 1.151 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.129 ms\n",
            "Performance= 235.24 GFlop/s, Time= 9.129 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 57.699 ms\n",
            "Performance= 297.75 GFlop/s, Time= 57.699 msec, Size= 17179869184 Ops\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *测试结果分析*"
      ],
      "metadata": {
        "id": "GNxmATlznP9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1. TILE_SIZE对性能的影响***\n",
        "\n",
        "- **TILE_SIZE=16**：\n",
        "  - 在较小的矩阵大小（128, 256）时，性能相对较低。\n",
        "  - 随着矩阵大小的增加，性能逐渐提升，但提升幅度不如TILE_SIZE=32和TILE_SIZE=64。\n",
        "\n",
        "- **TILE_SIZE=32**：\n",
        "  - 在所有测试的矩阵大小中，性能普遍高于TILE_SIZE=16。\n",
        "  - 在2048大小的矩阵中，性能达到最高，为306.83 GFlop/s。\n",
        "\n",
        "- **TILE_SIZE=64**：\n",
        "  - 性能略低于TILE_SIZE=32，但在某些情况下（如512和1024大小的矩阵）与TILE_SIZE=32相近。\n",
        "  - 在2048大小的矩阵中，性能为297.75 GFlop/s，略低于TILE_SIZE=32。\n",
        "\n",
        "***2. 矩阵大小对性能的影响***\n",
        "\n",
        "- **小矩阵（128, 256）**：\n",
        "  - 所有TILE_SIZE设置下，小矩阵的性能相对较低。\n",
        "  - 这可能是因为小矩阵的数据量不足以充分利用并行计算资源。\n",
        "\n",
        "- **中等矩阵（512, 1024）**：\n",
        "  - 性能显著提升，尤其是在TILE_SIZE=32和TILE_SIZE=64时。\n",
        "  - 这表明中等大小的矩阵能更好地利用并行计算资源。\n",
        "\n",
        "- **大矩阵（2048）**：\n",
        "  - 性能达到最高，尤其是在TILE_SIZE=32时。\n",
        "  - 大矩阵能充分利用并行计算资源，尤其是在较大的TILE_SIZE设置下。\n",
        "\n",
        "***3. 性能趋势和规律***\n",
        "\n",
        "- **性能提升与矩阵大小的关系**：\n",
        "  - 随着矩阵大小的增加，性能提升，尤其是在较大的TILE_SIZE设置下。\n",
        "  - 这表明并行计算核能够有效地扩展以处理更大的数据集。\n",
        "\n",
        "- **TILE_SIZE的最优值**：\n",
        "  - TILE_SIZE=32在大多数情况下提供了最佳性能，尤其是在处理大矩阵时。\n",
        "  - 这可能是因为较大的TILE_SIZE能更有效地利用缓存和并行处理能力。\n",
        "\n",
        "- **错误检查**：\n",
        "  - 在所有测试中，总错误数为0，表明核计算的结果与主机CPU的结果一致，验证了核的正确性。\n",
        "\n",
        "***4. 结论***\n",
        "\n",
        "- **最优TILE_SIZE选择**：TILE_SIZE=32在大多数情况下提供了最佳性能，尤其是在处理大矩阵时。\n",
        "- **矩阵大小的影响**：随着矩阵大小的增加，性能提升，表明并行计算核能够有效地扩展以处理更大的数据集。\n",
        "- **性能一致性**：核的计算结果与主机CPU的结果一致，验证了核的正确性。\n"
      ],
      "metadata": {
        "id": "WMNk1pbTnWBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## *Lab5*"
      ],
      "metadata": {
        "id": "3dL14M8ndhk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *实现 CUDA 共享内存矩阵计算优化*\n",
        "\n",
        "***Step 1: 定义共享内存数组***\n",
        "\n",
        "首先，我们需要在核函数中定义共享内存数组 `As` 和 `Bs`，它们将用于存储从全局内存中加载的矩阵 A 和 B 的子矩阵。\n",
        "\n",
        "```cpp\n",
        "__shared__ float As[TILE_WIDTH][TILE_WIDTH + 1];\n",
        "__shared__ float Bs[TILE_WIDTH][TILE_WIDTH + 1];\n",
        "```\n",
        "\n",
        "***Step 2: 计算线程的全局索引***\n",
        "\n",
        "接下来，我们需要计算每个线程在矩阵 A 和 B 中的全局索引。这将用于从全局内存中加载数据到共享内存。\n",
        "\n",
        "```cpp\n",
        "int aRow = wA * blockIdx.y + threadIdx.y;\n",
        "int aCol = threadIdx.x;\n",
        "int bRow = threadIdx.y;\n",
        "int bCol = wB * blockIdx.x + threadIdx.x;\n",
        "```\n",
        "\n",
        "***Step 3: 加载数据到共享内存***\n",
        "\n",
        "在加载数据之前，我们需要确保线程索引不会超出矩阵的边界。如果超出边界，我们将对应的共享内存元素设置为 0.0f。\n",
        "\n",
        "```cpp\n",
        "float a = (aRow < wA && aCol < wA) ? A[aRow * wA + aCol] : 0.0f;\n",
        "float b = (bRow < wA && bCol < wB) ? B[bRow * wB + bCol] : 0.0f;\n",
        "As[threadIdx.y][threadIdx.x] = a;\n",
        "Bs[threadIdx.y][threadIdx.x] = b;\n",
        "```\n",
        "\n",
        "***Step 4: 同步线程***\n",
        "\n",
        "在所有线程完成数据加载后，我们需要同步所有线程，以确保共享内存的数据已经准备好。\n",
        "\n",
        "```cpp\n",
        "__syncthreads();\n",
        "```\n",
        "\n",
        "***Step 5: 执行矩阵乘法***\n",
        "\n",
        "现在，我们可以执行矩阵乘法。每个线程将计算其负责的矩阵 C 的元素。\n",
        "\n",
        "```cpp\n",
        "float Csub = 0.0f;\n",
        "for (int k = 0; k < TILE_WIDTH; ++k) {\n",
        "    Csub += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
        "}\n",
        "```\n",
        "\n",
        "***Step 6: 存储结果到全局内存***\n",
        "\n",
        "最后，我们需要将计算得到的矩阵 C 的元素存储回全局内存。同样，我们需要确保线程索引不会超出矩阵的边界。\n",
        "\n",
        "```cpp\n",
        "int cRow = wA * blockIdx.y + threadIdx.y;\n",
        "int cCol = wB * blockIdx.x + threadIdx.x;\n",
        "if (cRow < wA && cCol < wB) {\n",
        "    C[cRow * wB + cCol] = Csub;\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "6c-QnATzuaQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***具体的代码实现如下：***\n",
        "\n",
        "```cuda\n",
        "__global__ void MatrixMulSharedMemKernel(float *A, float *B, float *C, int wA, int wB) {\n",
        "    // 块和线程的索引\n",
        "    int bx = blockIdx.x;\n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    // 初始化Csub变量\n",
        "    float Csub = 0;\n",
        "\n",
        "    // 计算A和B的子矩阵\n",
        "    for (int a = wA * TILE_WIDTH * by, b = TILE_WIDTH * bx; a < wA * TILE_WIDTH * by + wA - 1 && b < TILE_WIDTH * bx + wB - 1; a += TILE_WIDTH, b += TILE_WIDTH) {\n",
        "        __shared__ float As[TILE_WIDTH][TILE_WIDTH + 1];  // 增加1以减少银行冲突\n",
        "        __shared__ float Bs[TILE_WIDTH][TILE_WIDTH + 1];  // 增加1以减少银行冲突\n",
        "\n",
        "        // 加载子矩阵A到共享内存\n",
        "        int aRow = a / wA + ty;\n",
        "        int aCol = a % wA + tx;\n",
        "        if (aRow < wA && aCol < wA)\n",
        "            As[ty][tx] = A[aRow * wA + aCol];\n",
        "        else\n",
        "            As[ty][tx] = 0.0f;\n",
        "\n",
        "        // 加载子矩阵B到共享内存\n",
        "        int bRow = b / wB + ty;\n",
        "        int bCol = b % wB + tx;\n",
        "        if (bRow < wA && bCol < wB)\n",
        "            Bs[ty][tx] = B[bRow * wB + bCol];\n",
        "        else\n",
        "            Bs[ty][tx] = 0.0f;\n",
        "\n",
        "        __syncthreads();  // 在加载完子矩阵后同步\n",
        "\n",
        "        // 进行矩阵乘法\n",
        "        for (int k = 0; k < TILE_WIDTH; ++k) {\n",
        "            Csub += As[ty][k] * Bs[k][tx];\n",
        "        }\n",
        "\n",
        "        __syncthreads();  // 确保计算完成后再加载下一个子矩阵\n",
        "    }\n",
        "\n",
        "    // 存储结果到矩阵C\n",
        "    int row_C = by * TILE_WIDTH + ty;\n",
        "    int col_C = bx * TILE_WIDTH + tx;\n",
        "    if (row_C < wA && col_C < wB)\n",
        "        C[row_C * wB + col_C] = Csub;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "2iFAcyX32G1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *尝试编译*"
      ],
      "metadata": {
        "id": "8ZRC7mv6flKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=compute_75 -L/usr/local/cuda/lib64 -lcublas ./matrix_mul_lab5.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtDAXdCXYCSs",
        "outputId": "3c413216-ccc3-40f1-ce06-94598a51681b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab5.cu(176)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"alpha\"\u001b[0m was declared but never referenced\n",
            "      const float alpha = 1.0f;\n",
            "                  ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab5.cu(177)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"beta\"\u001b[0m was declared but never referenced\n",
            "      const float beta = 0.0f;\n",
            "                  ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *脚本测试*"
      ],
      "metadata": {
        "id": "o41TC6R1fwL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash testbench_lab5.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2JOcptxfuQJ",
        "outputId": "7c0b3f0e-0f6c-4af7-a929-c611384dbf53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---****** Testing with cuBLAS ******---\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=16\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=16\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.049 ms\n",
            "Performance= 84.92 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.165 ms\n",
            "Performance= 203.83 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 1.011 ms\n",
            "Performance= 265.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 7.813 ms\n",
            "Performance= 274.86 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 49.061 ms\n",
            "Performance= 350.17 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=32\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=32\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.062 ms\n",
            "Performance= 68.12 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.170 ms\n",
            "Performance= 197.29 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 1.029 ms\n",
            "Performance= 260.84 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 7.552 ms\n",
            "Performance= 284.37 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 54.768 ms\n",
            "Performance= 313.68 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=64\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=64\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 215.58 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.026 ms\n",
            "Performance= 1277.19 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.009 ms\n",
            "Performance= 30437.62 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.011 ms\n",
            "Performance= 199491.26 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.021 ms\n",
            "Performance= 802257.76 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "---****** Testing without cuBLAS ******---\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=16\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=16\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.048 ms\n",
            "Performance= 86.75 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.163 ms\n",
            "Performance= 206.31 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 1.009 ms\n",
            "Performance= 265.97 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 7.814 ms\n",
            "Performance= 274.84 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 48.784 ms\n",
            "Performance= 352.16 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=32\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=32\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.063 ms\n",
            "Performance= 66.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.168 ms\n",
            "Performance= 200.10 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 1.029 ms\n",
            "Performance= 260.92 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 7.557 ms\n",
            "Performance= 284.19 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 50.693 ms\n",
            "Performance= 338.90 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=64\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=64\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 222.61 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 1744.72 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.009 ms\n",
            "Performance= 28610.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.010 ms\n",
            "Performance= 222509.48 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.013 ms\n",
            "Performance= 1307527.68 GFlop/s\n",
            "--------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *测试结果分析*"
      ],
      "metadata": {
        "id": "JEgDI5lf3PBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1. **TILE_WIDTH的影响：**\n",
        "   - **TILE_WIDTH** 是优化GPU矩阵运算的重要参数，表示每个块的大小。通过设置适当的 `TILE_WIDTH`，可以平衡内存访问效率与计算资源的利用。\n",
        "   - 随着 **TILE_WIDTH** 的增大，性能在许多测试中有所提升，特别是当矩阵较大时，性能差异更为明显。\n",
        "\n",
        "* 2. **不同TILE_WIDTH的性能差异：**\n",
        "   - **TILE_WIDTH=16：** 对于较小的矩阵（例如 `128x128`），性能相对较高，但随着矩阵尺寸的增加，性能提升趋于平稳。整体来看，使用 `TILE_WIDTH=16` 的性能不是最优的，但适用于较小矩阵。\n",
        "   - **TILE_WIDTH=32：** 性能相较于 `TILE_WIDTH=16` 略有提升，但并不显著。适用于中等规模矩阵（如 `256x256`、`512x512`）。\n",
        "   - **TILE_WIDTH=64：** 性能大幅提升，特别是在大矩阵（如 `2048x2048`）时，性能提高显著。这个设置在矩阵较大时表现最好，内核执行时间减少，性能显著提高。\n",
        "\n",
        "* 3. **使用cuBLAS与不使用cuBLAS的差异：**\n",
        "   - **cuBLAS**（NVIDIA的BLAS库）显然提升了性能，尤其在较大矩阵时。比如在 `TILE_WIDTH=64` 时，使用 cuBLAS 可以大幅度提高性能，性能数值跳跃式地增加（从几百 GFlop/s 到百万级 GFlop/s）。\n",
        "   - 不使用 cuBLAS 时，性能会相对较低，尤其是在大矩阵时（如 `2048x2048`）。这表明 cuBLAS 在大规模矩阵乘法中能充分发挥GPU的计算能力和优化优势。\n",
        "\n",
        "* 4. **性能趋势：**\n",
        "   - **随着矩阵尺寸增大，性能提高**：在每种 `TILE_WIDTH` 下，随着矩阵大小从 128 增加到 2048，性能普遍呈现上升趋势。更大的矩阵可以更好地利用GPU的并行计算能力，减少了内存访问延迟和处理器空闲时间。\n",
        "   - **内核执行时间减少**：随着矩阵大小的增大，内核执行时间（`Kernel Elapsed Time`）逐渐减少，表明GPU计算更为高效。\n",
        "\n",
        "* 总结：\n",
        "    - 对于大矩阵计算，**TILE_WIDTH=64** 配合 **cuBLAS** 是最佳选择，提供了最高的性能。\n",
        "    - 不同 `TILE_WIDTH` 的选择主要取决于矩阵的大小，小矩阵选择小的 `TILE_WIDTH` 更为高效，而大矩阵则倾向于选择较大的 `TILE_WIDTH`。\n",
        "    - 使用 **cuBLAS** 可以显著提高计算性能，尤其在大矩阵时提升更为显著。\n",
        "\n"
      ],
      "metadata": {
        "id": "17nHlKgX3RVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## *对比分析结果*"
      ],
      "metadata": {
        "id": "5uJYE3WV4SY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过对比 **使用共享内存优化** 和 **未使用共享内存** 的GPU计算结果，可以观察到几项关键差异。\n",
        "\n",
        "* 1. **性能对比（GFlop/s）：**\n",
        "   - **使用共享内存优化的结果**：在矩阵较大时（如 `1024x1024`、`2048x2048`），性能显著提高。例如，在 `TILE_WIDTH=64` 时，性能达到了 **几百万 GFlop/s**，而在 `TILE_WIDTH=16` 时，性能只有 **几十到几百 GFlop/s**。\n",
        "   - **未使用共享内存的结果**：性能较低，尽管在 `TILE_SIZE=64` 设置下有一些提升，但总体的性能（特别是在大矩阵时）比共享内存优化的测试结果低很多。例如，`TILE_SIZE=64` 时，性能大致在 **200 GFlop/s 到 300 GFlop/s** 之间，远低于共享内存优化后的表现。\n",
        "\n",
        "* 2. **内核执行时间对比：**\n",
        "   - **使用共享内存优化的结果**：随着矩阵大小增加，内核执行时间（`Kernel Elapsed Time`）增加，但增幅较小，特别是在 `TILE_WIDTH=64` 时，内核执行时间大大减少。特别是在大矩阵（如 `2048x2048`）时，执行时间仅为 **0.02 ms**，远低于未使用共享内存的时间。\n",
        "   - **未使用共享内存的结果**：内核执行时间随着矩阵大小增加而显著增加。例如，在 `TILE_SIZE=64` 设置下，`2048x2048` 矩阵的内核执行时间为 **57.7 ms**，大大高于共享内存优化情况下的 **0.02 ms**。\n",
        "\n",
        "* 3. **TILE_SIZE的影响：**\n",
        "   - **TILE_SIZE=16**：在未使用共享内存的情况下，`TILE_SIZE=16` 显示出较为平稳的性能，尤其是对于较小的矩阵。对于较小矩阵（如 `128x128`），性能较好。\n",
        "   - **TILE_SIZE=32 和 TILE_SIZE=64**：随着 `TILE_SIZE` 增加，性能逐渐提升，尤其在较大矩阵时，`TILE_SIZE=64` 性能最高，但依旧比共享内存优化的结果低。\n",
        "\n",
        "* 4. **优化效果总结：**\n",
        "   - **共享内存优化**：通过使用共享内存，GPU可以减少对全局内存的访问次数，增加数据的本地性，提高内存访问效率。这对于大矩阵计算尤为重要，能够显著提高性能并减少内核执行时间。\n",
        "   - **未使用共享内存**：由于计算主要依赖于全局内存访问，导致内存访问效率较低，尤其是对于大矩阵时，性能提升有限，执行时间较长。\n",
        "\n",
        "* **结论：**\n",
        "    - 使用共享内存优化GPU代码显著提高了计算性能，尤其在大矩阵情况下，内核执行时间和性能差距非常明显。未使用共享内存时，虽然随着 `TILE_SIZE` 增加性能有所提升，但与共享内存优化的结果相比，差距依然明显。因此，**共享内存优化** 是大规模矩阵运算中不可忽视的重要提升手段。\n",
        "\n"
      ],
      "metadata": {
        "id": "tWpj0j7g4WaE"
      }
    }
  ]
}