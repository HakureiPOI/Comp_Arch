{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvpCrXACo4vvL8cMLhRfVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HakureiPOI/Comp_Arch/blob/lab4-5/Arch_Lab_4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***HakureiPOI*** *Arch_Lab_4-5*"
      ],
      "metadata": {
        "id": "PfEXjNjQNV9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 环境准备"
      ],
      "metadata": {
        "id": "X6szOjUDNgqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opQ3k9Irk3pU",
        "outputId": "bdb63439-fe49-4455-ccb4-3bbb1b1e2758"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKo3pTgOID9D",
        "outputId": "31f1fe3b-b475-4b76-c1d7-ce5018aec09c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 26 05:55:33 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di2mebnikhbw",
        "outputId": "aafa6711-afa4-4912-ddea-ec047721d03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Comp_Arch'...\n",
            "remote: Enumerating objects: 987, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 987 (delta 35), reused 44 (delta 7), pack-reused 900 (from 2)\u001b[K\n",
            "Receiving objects: 100% (987/987), 62.12 MiB | 17.01 MiB/s, done.\n",
            "Resolving deltas: 100% (604/604), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HakureiPOI/Comp_Arch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Comp_Arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTo4gTFQnGsL",
        "outputId": "5f13eb67-3655-4afb-b390-5ec5d31fa83d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Comp_Arch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp2kKX_YnRqC",
        "outputId": "62cf5acb-8dc6-43e4-a02d-357a1e7da7ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mmain\u001b[m\n",
            "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
            "  \u001b[31mremotes/origin/lab1\u001b[m\n",
            "  \u001b[31mremotes/origin/lab2\u001b[m\n",
            "  \u001b[31mremotes/origin/lab3\u001b[m\n",
            "  \u001b[31mremotes/origin/lab4-5\u001b[m\n",
            "  \u001b[31mremotes/origin/lab6\u001b[m\n",
            "  \u001b[31mremotes/origin/main\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git switch lab4-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgrwmtxSnUAC",
        "outputId": "87afcb52-4b86-4292-c4e9-65e279f4a5fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'lab4-5' set up to track remote branch 'lab4-5' from 'origin'.\n",
            "Switched to a new branch 'lab4-5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lab4-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-98u2u4knbtn",
        "outputId": "b7b9295e-e05c-4f31-d5b3-3755af6b591a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Comp_Arch/lab4-5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Lab4"
      ],
      "metadata": {
        "id": "czChKfb8ROdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 尝试编译"
      ],
      "metadata": {
        "id": "HtMKatquRStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=compute_75 -L/usr/local/cuda/lib64 -lcublas ./matrix_mul_lab4.cu"
      ],
      "metadata": {
        "id": "4EppDy2Ynzlh",
        "outputId": "70108fe3-412b-4652-e7ee-a55a2780f631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(198)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"alpha\"\u001b[0m was declared but never referenced\n",
            "    const float alpha = 1.0f;\n",
            "                ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(199)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"beta\"\u001b[0m was declared but never referenced\n",
            "    const float beta = 0.0f;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(18)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"BLOCK_SIZE\"\u001b[0m was declared but never referenced\n",
            "  const int BLOCK_SIZE = TILE_WIDTH;\n",
            "            ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash testbench_lab4.sh"
      ],
      "metadata": {
        "id": "W4aPosTqqV8O",
        "outputId": "db38a70e-53bb-43bf-d72a-a5d62bd740c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling with TILE_SIZE=16\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=16\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.061 ms\n",
            "Performance= 69.28 GFlop/s, Time= 0.061 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.176 ms\n",
            "Performance= 190.14 GFlop/s, Time= 0.176 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.149 ms\n",
            "Performance= 233.61 GFlop/s, Time= 1.149 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.124 ms\n",
            "Performance= 235.37 GFlop/s, Time= 9.124 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 58.307 ms\n",
            "Performance= 294.64 GFlop/s, Time= 58.307 msec, Size= 17179869184 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Compiling with TILE_SIZE=32\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=32\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.050 ms\n",
            "Performance= 83.16 GFlop/s, Time= 0.050 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.183 ms\n",
            "Performance= 183.22 GFlop/s, Time= 0.183 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.149 ms\n",
            "Performance= 233.57 GFlop/s, Time= 1.149 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.137 ms\n",
            "Performance= 235.04 GFlop/s, Time= 9.137 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 55.992 ms\n",
            "Performance= 306.83 GFlop/s, Time= 55.992 msec, Size= 17179869184 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Compiling with TILE_SIZE=64\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=64\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.051 ms\n",
            "Performance= 82.45 GFlop/s, Time= 0.051 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.179 ms\n",
            "Performance= 187.20 GFlop/s, Time= 0.179 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.151 ms\n",
            "Performance= 233.18 GFlop/s, Time= 1.151 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.129 ms\n",
            "Performance= 235.24 GFlop/s, Time= 9.129 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 57.699 ms\n",
            "Performance= 297.75 GFlop/s, Time= 57.699 msec, Size= 17179869184 Ops\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Lab5"
      ],
      "metadata": {
        "id": "3dL14M8ndhk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 尝试编译"
      ],
      "metadata": {
        "id": "8ZRC7mv6flKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=compute_75 -L/usr/local/cuda/lib64 -lcublas ./matrix_mul_lab5.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtDAXdCXYCSs",
        "outputId": "3c413216-ccc3-40f1-ce06-94598a51681b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab5.cu(176)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"alpha\"\u001b[0m was declared but never referenced\n",
            "      const float alpha = 1.0f;\n",
            "                  ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab5.cu(177)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"beta\"\u001b[0m was declared but never referenced\n",
            "      const float beta = 0.0f;\n",
            "                  ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 脚本测试"
      ],
      "metadata": {
        "id": "o41TC6R1fwL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash testbench_lab5.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2JOcptxfuQJ",
        "outputId": "7c0b3f0e-0f6c-4af7-a929-c611384dbf53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---****** Testing with cuBLAS ******---\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=16\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=16\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.049 ms\n",
            "Performance= 84.92 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.165 ms\n",
            "Performance= 203.83 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 1.011 ms\n",
            "Performance= 265.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 7.813 ms\n",
            "Performance= 274.86 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 49.061 ms\n",
            "Performance= 350.17 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=32\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=32\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.062 ms\n",
            "Performance= 68.12 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.170 ms\n",
            "Performance= 197.29 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 1.029 ms\n",
            "Performance= 260.84 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 7.552 ms\n",
            "Performance= 284.37 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 54.768 ms\n",
            "Performance= 313.68 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=64\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=64\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 215.58 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.026 ms\n",
            "Performance= 1277.19 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.009 ms\n",
            "Performance= 30437.62 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.011 ms\n",
            "Performance= 199491.26 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.021 ms\n",
            "Performance= 802257.76 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "---****** Testing without cuBLAS ******---\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=16\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=16\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.048 ms\n",
            "Performance= 86.75 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.163 ms\n",
            "Performance= 206.31 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 1.009 ms\n",
            "Performance= 265.97 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 7.814 ms\n",
            "Performance= 274.84 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 48.784 ms\n",
            "Performance= 352.16 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=32\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=32\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.063 ms\n",
            "Performance= 66.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.168 ms\n",
            "Performance= 200.10 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 1.029 ms\n",
            "Performance= 260.92 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 7.557 ms\n",
            "Performance= 284.19 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 50.693 ms\n",
            "Performance= 338.90 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=64\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=64\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 222.61 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 1744.72 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.009 ms\n",
            "Performance= 28610.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.010 ms\n",
            "Performance= 222509.48 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.013 ms\n",
            "Performance= 1307527.68 GFlop/s\n",
            "--------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGA2xXHwf8cK"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}