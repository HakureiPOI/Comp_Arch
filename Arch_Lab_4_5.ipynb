{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvpCrXACo4vvL8cMLhRfVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HakureiPOI/Comp_Arch/blob/lab4-5/Arch_Lab_4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***HakureiPOI*** *Arch_Lab_4-5*"
      ],
      "metadata": {
        "id": "PfEXjNjQNV9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 环境准备"
      ],
      "metadata": {
        "id": "X6szOjUDNgqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opQ3k9Irk3pU",
        "outputId": "fe404b07-8395-476b-9473-b2c013fbcf3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKo3pTgOID9D",
        "outputId": "4f60cabd-ad2e-404b-e2a9-6faff8048518"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 25 07:27:28 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di2mebnikhbw",
        "outputId": "de7282be-9df3-4f15-bebe-c733e4b598ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Comp_Arch'...\n",
            "remote: Enumerating objects: 893, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 893 (delta 6), reused 0 (delta 0), pack-reused 879 (from 1)\u001b[K\n",
            "Receiving objects: 100% (893/893), 6.43 MiB | 13.56 MiB/s, done.\n",
            "Resolving deltas: 100% (569/569), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HakureiPOI/Comp_Arch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Comp_Arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTo4gTFQnGsL",
        "outputId": "cea59b5d-92c9-4334-cca7-c246a023e301"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Comp_Arch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp2kKX_YnRqC",
        "outputId": "aaa0973a-77c8-40fb-9cd7-8be634e1872c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mmain\u001b[m\n",
            "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
            "  \u001b[31mremotes/origin/lab1\u001b[m\n",
            "  \u001b[31mremotes/origin/lab2\u001b[m\n",
            "  \u001b[31mremotes/origin/lab3\u001b[m\n",
            "  \u001b[31mremotes/origin/lab4-5\u001b[m\n",
            "  \u001b[31mremotes/origin/main\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git switch lab4-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgrwmtxSnUAC",
        "outputId": "a539b450-ba12-4930-e941-cc60736408f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'lab4-5' set up to track remote branch 'lab4-5' from 'origin'.\n",
            "Switched to a new branch 'lab4-5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lab4-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-98u2u4knbtn",
        "outputId": "98fe6129-6dd5-430b-bd33-838fd0db89be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Comp_Arch/lab4-5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Lab4"
      ],
      "metadata": {
        "id": "czChKfb8ROdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 尝试编译"
      ],
      "metadata": {
        "id": "HtMKatquRStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=compute_75 -L/usr/local/cuda/lib64 -lcublas ./matrix_mul_lab4.cu"
      ],
      "metadata": {
        "id": "4EppDy2Ynzlh",
        "outputId": "e33e7fdd-13b2-48a9-a011-6c92a4f1e056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(198)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"alpha\"\u001b[0m was declared but never referenced\n",
            "    const float alpha = 1.0f;\n",
            "                ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(199)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"beta\"\u001b[0m was declared but never referenced\n",
            "    const float beta = 0.0f;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(18)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"BLOCK_SIZE\"\u001b[0m was declared but never referenced\n",
            "  const int BLOCK_SIZE = TILE_WIDTH;\n",
            "            ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash testbench_lab4.sh"
      ],
      "metadata": {
        "id": "W4aPosTqqV8O",
        "outputId": "65aa8ea9-5128-436c-c64a-935581c3602a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling with TILE_SIZE=16\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=16\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.061 ms\n",
            "Performance= 68.42 GFlop/s, Time= 0.061 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.188 ms\n",
            "Performance= 178.11 GFlop/s, Time= 0.188 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.156 ms\n",
            "Performance= 232.21 GFlop/s, Time= 1.156 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.131 ms\n",
            "Performance= 235.19 GFlop/s, Time= 9.131 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 63.919 ms\n",
            "Performance= 268.77 GFlop/s, Time= 63.919 msec, Size= 17179869184 Ops\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Lab5"
      ],
      "metadata": {
        "id": "3dL14M8ndhk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 尝试编译"
      ],
      "metadata": {
        "id": "8ZRC7mv6flKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=compute_75 -L/usr/local/cuda/lib64 -lcublas ./matrix_mul_lab5.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtDAXdCXYCSs",
        "outputId": "d04a7cdd-5f69-45e9-da71-7052fa9792af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab5.cu(176)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"alpha\"\u001b[0m was declared but never referenced\n",
            "      const float alpha = 1.0f;\n",
            "                  ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab5.cu(177)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"beta\"\u001b[0m was declared but never referenced\n",
            "      const float beta = 0.0f;\n",
            "                  ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 脚本测试"
      ],
      "metadata": {
        "id": "o41TC6R1fwL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash testbench_lab5.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2JOcptxfuQJ",
        "outputId": "e29ada80-e451-4837-c0cc-c946de815112"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---****** Testing with cuBLAS ******---\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=16\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=16\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.050 ms\n",
            "Performance= 84.38 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.164 ms\n",
            "Performance= 205.15 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 1.009 ms\n",
            "Performance= 266.09 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 7.805 ms\n",
            "Performance= 275.15 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 56.086 ms\n",
            "Performance= 306.31 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=32\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=32\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.068 ms\n",
            "Performance= 62.01 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.176 ms\n",
            "Performance= 190.59 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 1.032 ms\n",
            "Performance= 260.05 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 7.565 ms\n",
            "Performance= 283.86 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 56.594 ms\n",
            "Performance= 303.56 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=64\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=64\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.023 ms\n",
            "Performance= 185.13 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.027 ms\n",
            "Performance= 1248.60 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.016 ms\n",
            "Performance= 17203.87 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.014 ms\n",
            "Performance= 149329.91 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 923093.05 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "---****** Testing without cuBLAS ******---\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=16\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=16\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.050 ms\n",
            "Performance= 84.63 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.166 ms\n",
            "Performance= 201.77 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 1.009 ms\n",
            "Performance= 266.06 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 7.807 ms\n",
            "Performance= 275.08 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 49.301 ms\n",
            "Performance= 348.47 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=32\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=32\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.060 ms\n",
            "Performance= 69.67 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.173 ms\n",
            "Performance= 194.26 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 1.025 ms\n",
            "Performance= 261.87 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 7.551 ms\n",
            "Performance= 284.38 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 48.737 ms\n",
            "Performance= 352.50 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=64\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=64\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.021 ms\n",
            "Performance= 200.78 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.022 ms\n",
            "Performance= 1537.95 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.009 ms\n",
            "Performance= 29026.33 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.013 ms\n",
            "Performance= 169381.28 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.029 ms\n",
            "Performance= 601199.26 GFlop/s\n",
            "--------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGA2xXHwf8cK"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}