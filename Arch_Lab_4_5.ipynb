{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMOQilNUJFyHn5bnxIS8H/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HakureiPOI/Comp_Arch/blob/lab4-5/Arch_Lab_4_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***HakureiPOI*** *Arch_Lab_4-5*"
      ],
      "metadata": {
        "id": "PfEXjNjQNV9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## *环境准备*"
      ],
      "metadata": {
        "id": "X6szOjUDNgqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opQ3k9Irk3pU",
        "outputId": "bdb63439-fe49-4455-ccb4-3bbb1b1e2758"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKo3pTgOID9D",
        "outputId": "31f1fe3b-b475-4b76-c1d7-ce5018aec09c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 26 05:55:33 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di2mebnikhbw",
        "outputId": "aafa6711-afa4-4912-ddea-ec047721d03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Comp_Arch'...\n",
            "remote: Enumerating objects: 987, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 987 (delta 35), reused 44 (delta 7), pack-reused 900 (from 2)\u001b[K\n",
            "Receiving objects: 100% (987/987), 62.12 MiB | 17.01 MiB/s, done.\n",
            "Resolving deltas: 100% (604/604), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HakureiPOI/Comp_Arch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Comp_Arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTo4gTFQnGsL",
        "outputId": "5f13eb67-3655-4afb-b390-5ec5d31fa83d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Comp_Arch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp2kKX_YnRqC",
        "outputId": "62cf5acb-8dc6-43e4-a02d-357a1e7da7ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mmain\u001b[m\n",
            "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
            "  \u001b[31mremotes/origin/lab1\u001b[m\n",
            "  \u001b[31mremotes/origin/lab2\u001b[m\n",
            "  \u001b[31mremotes/origin/lab3\u001b[m\n",
            "  \u001b[31mremotes/origin/lab4-5\u001b[m\n",
            "  \u001b[31mremotes/origin/lab6\u001b[m\n",
            "  \u001b[31mremotes/origin/main\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git switch lab4-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgrwmtxSnUAC",
        "outputId": "87afcb52-4b86-4292-c4e9-65e279f4a5fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'lab4-5' set up to track remote branch 'lab4-5' from 'origin'.\n",
            "Switched to a new branch 'lab4-5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lab4-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-98u2u4knbtn",
        "outputId": "b7b9295e-e05c-4f31-d5b3-3755af6b591a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Comp_Arch/lab4-5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## *Lab4*"
      ],
      "metadata": {
        "id": "czChKfb8ROdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### *实现 CUDA 矩阵乘法核函数*\n",
        "\n",
        "* Step 1: 计算行和列索引\n",
        "在核函数中，首先需要计算每个线程负责计算的输出矩阵 `P` 的元素位置。这通过 `blockIdx` 和 `threadIdx` 来实现，它们分别表示线程块和线程在网格中的位置。\n",
        "\n",
        "```cuda\n",
        "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "```\n",
        "\n",
        "这里，`blockDim` 定义了每个线程块的尺寸，`blockIdx` 和 `threadIdx` 用于计算每个线程的全局位置。\n",
        "\n",
        "* Step 2: 确保线程在矩阵边界内\n",
        "在进行计算之前，需要确保线程计算的位置在矩阵的边界内，以避免越界访问。\n",
        "\n",
        "```cuda\n",
        "if (row < width && col < width) {\n",
        "    float pValue = 0.0;\n",
        "```\n",
        "\n",
        "* Step 3: 计算矩阵乘法的单个元素\n",
        "每个线程计算矩阵 `P` 的一个元素，这是通过遍历矩阵 `M` 的列（或矩阵 `N` 的行）来实现的。\n",
        "\n",
        "```cuda\n",
        "for (int k = 0; k < width; ++k) {\n",
        "    pValue += d_M[row * width + k] * d_N[k * width + col];\n",
        "}\n",
        "```\n",
        "\n",
        "这里，`d_M` 和 `d_N` 分别是输入矩阵 `M` 和 `N` 在设备上的存储，`width` 是矩阵的宽度（对于方阵来说，宽度等于高度）。\n",
        "\n",
        "* Step 4: 存储计算结果\n",
        "计算完成后，将结果存储到输出矩阵 `P` 的相应位置。\n",
        "\n",
        "```cuda\n",
        "d_P[row * width + col] = pValue;\n",
        "```\n"
      ],
      "metadata": {
        "id": "iZucsMHplTQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "具体的代码实现如下\n",
        "\n",
        "\n",
        "```\n",
        "__global__ void MatrixMulKernel(float *d_M, float *d_N, float *d_P, int width) {\n",
        "  // Calculate the row index of the P element and M\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "  // Calculate the column index of the P element and N\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  // Ensure the thread is within bounds\n",
        "  if (row < width && col < width) {\n",
        "    float pValue = 0.0;\n",
        "\n",
        "    // Each thread computes one element of the matrix\n",
        "    for (int k = 0; k < width; ++k) {\n",
        "      pValue += d_M[row * width + k] * d_N[k * width + col];\n",
        "    }\n",
        "\n",
        "    // Store the computed value into the output matrix\n",
        "    d_P[row * width + col] = pValue;\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "KKQGfo6al_j6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *尝试编译*"
      ],
      "metadata": {
        "id": "HtMKatquRStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=compute_75 -L/usr/local/cuda/lib64 -lcublas ./matrix_mul_lab4.cu"
      ],
      "metadata": {
        "id": "4EppDy2Ynzlh",
        "outputId": "70108fe3-412b-4652-e7ee-a55a2780f631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(198)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"alpha\"\u001b[0m was declared but never referenced\n",
            "    const float alpha = 1.0f;\n",
            "                ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(199)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"beta\"\u001b[0m was declared but never referenced\n",
            "    const float beta = 0.0f;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab4.cu(18)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"BLOCK_SIZE\"\u001b[0m was declared but never referenced\n",
            "  const int BLOCK_SIZE = TILE_WIDTH;\n",
            "            ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *脚本测试*"
      ],
      "metadata": {
        "id": "oyRzPxhpnRjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash testbench_lab4.sh"
      ],
      "metadata": {
        "id": "W4aPosTqqV8O",
        "outputId": "db38a70e-53bb-43bf-d72a-a5d62bd740c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling with TILE_SIZE=16\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=16\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.061 ms\n",
            "Performance= 69.28 GFlop/s, Time= 0.061 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.176 ms\n",
            "Performance= 190.14 GFlop/s, Time= 0.176 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.149 ms\n",
            "Performance= 233.61 GFlop/s, Time= 1.149 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.124 ms\n",
            "Performance= 235.37 GFlop/s, Time= 9.124 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 58.307 ms\n",
            "Performance= 294.64 GFlop/s, Time= 58.307 msec, Size= 17179869184 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Compiling with TILE_SIZE=32\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=32\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.050 ms\n",
            "Performance= 83.16 GFlop/s, Time= 0.050 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.183 ms\n",
            "Performance= 183.22 GFlop/s, Time= 0.183 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.149 ms\n",
            "Performance= 233.57 GFlop/s, Time= 1.149 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.137 ms\n",
            "Performance= 235.04 GFlop/s, Time= 9.137 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 55.992 ms\n",
            "Performance= 306.83 GFlop/s, Time= 55.992 msec, Size= 17179869184 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Compiling with TILE_SIZE=64\n",
            "=======================================\n",
            "Running tests with TILE_SIZE=64\n",
            "=======================================\n",
            "Matrix size: 128\n",
            "Kernel Elpased Time: 0.051 ms\n",
            "Performance= 82.45 GFlop/s, Time= 0.051 msec, Size= 4194304 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 256\n",
            "Kernel Elpased Time: 0.179 ms\n",
            "Performance= 187.20 GFlop/s, Time= 0.179 msec, Size= 33554432 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 512\n",
            "Kernel Elpased Time: 1.151 ms\n",
            "Performance= 233.18 GFlop/s, Time= 1.151 msec, Size= 268435456 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 1024\n",
            "Kernel Elpased Time: 9.129 ms\n",
            "Performance= 235.24 GFlop/s, Time= 9.129 msec, Size= 2147483648 Ops\n",
            "Computing result using host CPU...done.\n",
            "Listing first 100 Differences > 0.000010...\n",
            " \n",
            "  Total Errors = 0\n",
            "--------------------------------\n",
            "\n",
            "Matrix size: 2048\n",
            "Kernel Elpased Time: 57.699 ms\n",
            "Performance= 297.75 GFlop/s, Time= 57.699 msec, Size= 17179869184 Ops\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *测试结果分析*"
      ],
      "metadata": {
        "id": "GNxmATlznP9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1. TILE_SIZE对性能的影响***\n",
        "\n",
        "- **TILE_SIZE=16**：\n",
        "  - 在较小的矩阵大小（128, 256）时，性能相对较低。\n",
        "  - 随着矩阵大小的增加，性能逐渐提升，但提升幅度不如TILE_SIZE=32和TILE_SIZE=64。\n",
        "\n",
        "- **TILE_SIZE=32**：\n",
        "  - 在所有测试的矩阵大小中，性能普遍高于TILE_SIZE=16。\n",
        "  - 在2048大小的矩阵中，性能达到最高，为306.83 GFlop/s。\n",
        "\n",
        "- **TILE_SIZE=64**：\n",
        "  - 性能略低于TILE_SIZE=32，但在某些情况下（如512和1024大小的矩阵）与TILE_SIZE=32相近。\n",
        "  - 在2048大小的矩阵中，性能为297.75 GFlop/s，略低于TILE_SIZE=32。\n",
        "\n",
        "***2. 矩阵大小对性能的影响***\n",
        "\n",
        "- **小矩阵（128, 256）**：\n",
        "  - 所有TILE_SIZE设置下，小矩阵的性能相对较低。\n",
        "  - 这可能是因为小矩阵的数据量不足以充分利用并行计算资源。\n",
        "\n",
        "- **中等矩阵（512, 1024）**：\n",
        "  - 性能显著提升，尤其是在TILE_SIZE=32和TILE_SIZE=64时。\n",
        "  - 这表明中等大小的矩阵能更好地利用并行计算资源。\n",
        "\n",
        "- **大矩阵（2048）**：\n",
        "  - 性能达到最高，尤其是在TILE_SIZE=32时。\n",
        "  - 大矩阵能充分利用并行计算资源，尤其是在较大的TILE_SIZE设置下。\n",
        "\n",
        "***3. 性能趋势和规律***\n",
        "\n",
        "- **性能提升与矩阵大小的关系**：\n",
        "  - 随着矩阵大小的增加，性能提升，尤其是在较大的TILE_SIZE设置下。\n",
        "  - 这表明并行计算核能够有效地扩展以处理更大的数据集。\n",
        "\n",
        "- **TILE_SIZE的最优值**：\n",
        "  - TILE_SIZE=32在大多数情况下提供了最佳性能，尤其是在处理大矩阵时。\n",
        "  - 这可能是因为较大的TILE_SIZE能更有效地利用缓存和并行处理能力。\n",
        "\n",
        "- **错误检查**：\n",
        "  - 在所有测试中，总错误数为0，表明核计算的结果与主机CPU的结果一致，验证了核的正确性。\n",
        "\n",
        "***4. 结论***\n",
        "\n",
        "- **最优TILE_SIZE选择**：TILE_SIZE=32在大多数情况下提供了最佳性能，尤其是在处理大矩阵时。\n",
        "- **矩阵大小的影响**：随着矩阵大小的增加，性能提升，表明并行计算核能够有效地扩展以处理更大的数据集。\n",
        "- **性能一致性**：核的计算结果与主机CPU的结果一致，验证了核的正确性。\n"
      ],
      "metadata": {
        "id": "WMNk1pbTnWBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Lab5"
      ],
      "metadata": {
        "id": "3dL14M8ndhk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 尝试编译"
      ],
      "metadata": {
        "id": "8ZRC7mv6flKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=compute_75 -L/usr/local/cuda/lib64 -lcublas ./matrix_mul_lab5.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtDAXdCXYCSs",
        "outputId": "3c413216-ccc3-40f1-ce06-94598a51681b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab5.cu(176)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"alpha\"\u001b[0m was declared but never referenced\n",
            "      const float alpha = 1.0f;\n",
            "                  ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m./matrix_mul_lab5.cu(177)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"beta\"\u001b[0m was declared but never referenced\n",
            "      const float beta = 0.0f;\n",
            "                  ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 脚本测试"
      ],
      "metadata": {
        "id": "o41TC6R1fwL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash testbench_lab5.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2JOcptxfuQJ",
        "outputId": "7c0b3f0e-0f6c-4af7-a929-c611384dbf53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---****** Testing with cuBLAS ******---\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=16\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=16\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.049 ms\n",
            "Performance= 84.92 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.165 ms\n",
            "Performance= 203.83 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 1.011 ms\n",
            "Performance= 265.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 7.813 ms\n",
            "Performance= 274.86 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 49.061 ms\n",
            "Performance= 350.17 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=32\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=32\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.062 ms\n",
            "Performance= 68.12 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.170 ms\n",
            "Performance= 197.29 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 1.029 ms\n",
            "Performance= 260.84 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 7.552 ms\n",
            "Performance= 284.37 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 54.768 ms\n",
            "Performance= 313.68 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=64\n",
            "=======================================\n",
            "use cublas with TILE_WIDTH=64\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 215.58 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.026 ms\n",
            "Performance= 1277.19 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.009 ms\n",
            "Performance= 30437.62 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.011 ms\n",
            "Performance= 199491.26 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.021 ms\n",
            "Performance= 802257.76 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "---****** Testing without cuBLAS ******---\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=16\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=16\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.048 ms\n",
            "Performance= 86.75 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 0.163 ms\n",
            "Performance= 206.31 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 1.009 ms\n",
            "Performance= 265.97 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 7.814 ms\n",
            "Performance= 274.84 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 16\n",
            "Kernel Elapsed Time: 48.784 ms\n",
            "Performance= 352.16 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=32\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=32\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.063 ms\n",
            "Performance= 66.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 0.168 ms\n",
            "Performance= 200.10 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 1.029 ms\n",
            "Performance= 260.92 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 7.557 ms\n",
            "Performance= 284.19 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 32\n",
            "Kernel Elapsed Time: 50.693 ms\n",
            "Performance= 338.90 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "=======================================\n",
            "Testing with TILE_WIDTH=64\n",
            "=======================================\n",
            "use no cublas with TILE_WIDTH=64\n",
            "\n",
            "matrix_size: 128, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 222.61 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 256, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.019 ms\n",
            "Performance= 1744.72 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 512, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.009 ms\n",
            "Performance= 28610.53 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 1024, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.010 ms\n",
            "Performance= 222509.48 GFlop/s\n",
            "--------------------------------\n",
            "\n",
            "matrix_size: 2048, TILE_WIDTH: 64\n",
            "Kernel Elapsed Time: 0.013 ms\n",
            "Performance= 1307527.68 GFlop/s\n",
            "--------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGA2xXHwf8cK"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}